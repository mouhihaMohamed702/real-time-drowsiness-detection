# Drowsiness Detection Project


```python
import tensorflow as tf ## pip install tensorflow-gpu
import cv2 ### pip install opencv-python
# pip install opencv-contrib-python fullpackage
import os
import matplotlib.pyplot as plt ## pip install matlplotlib
import numpy as np ## pip install numpy
```


```python
ing_array = cv2.imread("prepared_Data/test/Closed_Eyes/s0001_00784_0_0_0_0_0_01.png", cv2.IMREAD_GRAYSCALE)
```


```python
plt.imshow(ing_array, cmap="gray")

```




    <matplotlib.image.AxesImage at 0x23d70009bb0>




    
![png](output_3_1.png)
    



```python
from keras.preprocessing.image import ImageDataGenerator
```


```python
from keras.preprocessing import image
```


```python
plt.imshow(ing_array, cmap="gray")

```




    <matplotlib.image.AxesImage at 0x23d711011f0>




    
![png](output_6_1.png)
    



```python
ing_array.shape # the shape of an array is the number of elements in each dimension
```




    (82, 82)




```python
Datadirectory = "prepared_Data/test/" ##  training Ä‘ataset
Classes  = ["closed_Eyes", "open_Eyes"] ## List of classes
for category in Classes:
    path = os.path.join(Datadirectory, category) ## //
    for ing in os.listdir(path):
        ing_array = cv2.imread(os.path.join(path, ing), cv2.IMREAD_GRAYSCALE)
        backtorgb = cv2.cvtColor(ing_array,cv2.COLOR_GRAY2RGB)
        plt.imshow(ing_array, cmap="gray")
        plt.show()
        break
    break
```


    
![png](output_8_0.png)
    



```python
ing_size= 224
new_array= cv2.resize(backtorgb, (ing_size,ing_size))
plt.imshow(new_array, cmap="gray")
plt.show()
```


    
![png](output_9_0.png)
    


# reading all the images and converting them into an array for data and labels


```python
training_Data = []
def create_training_Data():
    for category in Classes:
        path = os.path.join(Datadirectory, category)
        class_num = Classes.index(category) ## e 1,
        for ing in os.listdir(path):
            try:
                ing_array = cv2.imread(os.path.join(path,ing), cv2.IMREAD_GRAYSCALE)
                backtorgb = cv2.cvtColor(ing_array,cv2.COLOR_GRAY2RGB)
                new_array= cv2.resize(backtorgb, (ing_size, ing_size))
                training_Data.append([new_array,class_num])
            except Exception as e:
                pass
```


```python
print(path)
```

    prepared_Data/test/closed_Eyes
    


```python
create_training_Data()

```


```python
print(new_array)
```

    [[[ 50  50  50]
      [ 50  50  50]
      [ 49  49  49]
      ...
      [104 104 104]
      [104 104 104]
      [104 104 104]]
    
     [[ 50  50  50]
      [ 50  50  50]
      [ 50  50  50]
      ...
      [104 104 104]
      [104 104 104]
      [104 104 104]]
    
     [[ 50  50  50]
      [ 50  50  50]
      [ 50  50  50]
      ...
      [104 104 104]
      [104 104 104]
      [104 104 104]]
    
     ...
    
     [[ 63  63  63]
      [ 63  63  63]
      [ 64  64  64]
      ...
      [ 83  83  83]
      [ 83  83  83]
      [ 83  83  83]]
    
     [[ 63  63  63]
      [ 63  63  63]
      [ 64  64  64]
      ...
      [ 82  82  82]
      [ 82  82  82]
      [ 82  82  82]]
    
     [[ 63  63  63]
      [ 63  63  63]
      [ 64  64  64]
      ...
      [ 81  81  81]
      [ 82  82  82]
      [ 82  82  82]]]
    


```python
print(len(training_Data ))
```

    2502
    


```python
import random
random.shuffle(training_Data)
```


```python
X = []
y = []
for features,label in training_Data:
    X.append(features)
    y.append(label)
X = np.array(X).reshape(-1, ing_size, ing_size, 3)
```


```python
X.shape
```




    (2502, 224, 224, 3)



# normalize the data
X= X/255.0; ## we are normalizing it


```python
len(X)
```




    2502




```python
len(y)
```




    2502



# test


```python
from sklearn.model_selection import train_test_split
```


```python

train_x,test_x,train_y,test_y=train_test_split(X,y,test_size=0.2,random_state=0)
```

# test


```python

```


```python
prediction = model.predict_classes(test_x)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    C:\Users\MOUHIH~1\AppData\Local\Temp/ipykernel_10548/3242923613.py in <module>
    ----> 1 prediction = model.predict_classes(test_x)
    

    NameError: name 'model' is not defined



```python
Y=np.array(y)
X=np.array(X)

```


```python
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
Y_train = to_categorical(Y_train)
Y_test = to_categorical(Y_test)
```


```python
import pickle
pickle_out = open("X.pickle","wb")
pickle.dump(X, pickle_out)
pickle_out.close()
pickle_out = open("y.pickle", "wb")
pickle.dump(y, pickle_out)
pickle_out.close()
            
```


```python
pickle_in = open("X.pickle", "rb")
X = pickle.load (pickle_in)
pickle_in = open ("y.pickle", "rb")
y = pickle.load(pickle_in)
```


```python
y
```




    [0,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     0,
     1,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     0,
     0,
     1,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     0,
     0,
     0,
     1,
     0,
     1,
     0,
     0,
     1,
     1,
     0,
     1,
     0,
     1,
     1,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     1,
     0,
     0,
     0,
     0,
     1,
     1,
     1,
     0,
     1,
     1,
     0,
     1,
     1,
     1,
     0,
     1,
     0,
     1,
     0,
     1,
     1,
     0,
     ...]



# deep learning model for training - Training Learning



```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
```


```python
model = tf.keras.applications.mobilenet.MobileNet()
```


```python
model.summary()
```

    Model: "mobilenet_1.00_224"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 224, 224, 3)]     0         
    _________________________________________________________________
    conv1 (Conv2D)               (None, 112, 112, 32)      864       
    _________________________________________________________________
    conv1_bn (BatchNormalization (None, 112, 112, 32)      128       
    _________________________________________________________________
    conv1_relu (ReLU)            (None, 112, 112, 32)      0         
    _________________________________________________________________
    conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       
    _________________________________________________________________
    conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       
    _________________________________________________________________
    conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         
    _________________________________________________________________
    conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      
    _________________________________________________________________
    conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       
    _________________________________________________________________
    conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         
    _________________________________________________________________
    conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         
    _________________________________________________________________
    conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       
    _________________________________________________________________
    conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       
    _________________________________________________________________
    conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         
    _________________________________________________________________
    conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      
    _________________________________________________________________
    conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       
    _________________________________________________________________
    conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         
    _________________________________________________________________
    conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      
    _________________________________________________________________
    conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       
    _________________________________________________________________
    conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         
    _________________________________________________________________
    conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     
    _________________________________________________________________
    conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       
    _________________________________________________________________
    conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         
    _________________________________________________________________
    conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         
    _________________________________________________________________
    conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      
    _________________________________________________________________
    conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       
    _________________________________________________________________
    conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         
    _________________________________________________________________
    conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     
    _________________________________________________________________
    conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      
    _________________________________________________________________
    conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         
    _________________________________________________________________
    conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      
    _________________________________________________________________
    conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      
    _________________________________________________________________
    conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         
    _________________________________________________________________
    conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     
    _________________________________________________________________
    conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      
    _________________________________________________________________
    conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         
    _________________________________________________________________
    conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         
    _________________________________________________________________
    conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      
    _________________________________________________________________
    conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      
    _________________________________________________________________
    conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         
    _________________________________________________________________
    conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    
    _________________________________________________________________
    conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         
    _________________________________________________________________
    conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      
    _________________________________________________________________
    conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      
    _________________________________________________________________
    conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         
    _________________________________________________________________
    conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    
    _________________________________________________________________
    conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      
    _________________________________________________________________
    conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         
    _________________________________________________________________
    conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      
    _________________________________________________________________
    conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      
    _________________________________________________________________
    conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         
    _________________________________________________________________
    conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   
    _________________________________________________________________
    conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      
    _________________________________________________________________
    conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         
    _________________________________________________________________
    global_average_pooling2d (Gl (None, 1024)              0         
    _________________________________________________________________
    reshape_1 (Reshape)          (None, 1, 1, 1024)        0         
    _________________________________________________________________
    dropout (Dropout)            (None, 1, 1, 1024)        0         
    _________________________________________________________________
    conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   
    _________________________________________________________________
    reshape_2 (Reshape)          (None, 1000)              0         
    _________________________________________________________________
    predictions (Activation)     (None, 1000)              0         
    =================================================================
    Total params: 4,253,864
    Trainable params: 4,231,976
    Non-trainable params: 21,888
    _________________________________________________________________
    

# transfer learning


```python
base_input = model.layers[0].input ## input
```


```python
base_output = model.layers[-4].output 
```


```python
Flat_layer = layers.Flatten()(base_output)
final_output = layers.Dense(1)(Flat_layer) ## one node (1/0)
final_ouput = layers.Activation('sigmoid')(final_output)
```


```python
new_model = keras.Model(inputs = base_input, outputs= final_output)

```


```python
new_model.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 224, 224, 3)]     0         
    _________________________________________________________________
    conv1 (Conv2D)               (None, 112, 112, 32)      864       
    _________________________________________________________________
    conv1_bn (BatchNormalization (None, 112, 112, 32)      128       
    _________________________________________________________________
    conv1_relu (ReLU)            (None, 112, 112, 32)      0         
    _________________________________________________________________
    conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       
    _________________________________________________________________
    conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       
    _________________________________________________________________
    conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         
    _________________________________________________________________
    conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      
    _________________________________________________________________
    conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       
    _________________________________________________________________
    conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         
    _________________________________________________________________
    conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         
    _________________________________________________________________
    conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       
    _________________________________________________________________
    conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       
    _________________________________________________________________
    conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         
    _________________________________________________________________
    conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      
    _________________________________________________________________
    conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       
    _________________________________________________________________
    conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         
    _________________________________________________________________
    conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      
    _________________________________________________________________
    conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       
    _________________________________________________________________
    conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         
    _________________________________________________________________
    conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     
    _________________________________________________________________
    conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       
    _________________________________________________________________
    conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         
    _________________________________________________________________
    conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         
    _________________________________________________________________
    conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      
    _________________________________________________________________
    conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       
    _________________________________________________________________
    conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         
    _________________________________________________________________
    conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     
    _________________________________________________________________
    conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      
    _________________________________________________________________
    conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         
    _________________________________________________________________
    conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      
    _________________________________________________________________
    conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      
    _________________________________________________________________
    conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         
    _________________________________________________________________
    conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     
    _________________________________________________________________
    conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      
    _________________________________________________________________
    conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         
    _________________________________________________________________
    conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         
    _________________________________________________________________
    conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      
    _________________________________________________________________
    conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      
    _________________________________________________________________
    conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         
    _________________________________________________________________
    conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    
    _________________________________________________________________
    conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      
    _________________________________________________________________
    conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    
    _________________________________________________________________
    conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      
    _________________________________________________________________
    conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         
    _________________________________________________________________
    conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         
    _________________________________________________________________
    conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      
    _________________________________________________________________
    conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      
    _________________________________________________________________
    conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         
    _________________________________________________________________
    conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    
    _________________________________________________________________
    conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      
    _________________________________________________________________
    conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         
    _________________________________________________________________
    conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      
    _________________________________________________________________
    conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      
    _________________________________________________________________
    conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         
    _________________________________________________________________
    conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   
    _________________________________________________________________
    conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      
    _________________________________________________________________
    conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         
    _________________________________________________________________
    global_average_pooling2d (Gl (None, 1024)              0         
    _________________________________________________________________
    reshape_1 (Reshape)          (None, 1, 1, 1024)        0         
    _________________________________________________________________
    dropout (Dropout)            (None, 1, 1, 1024)        0         
    _________________________________________________________________
    flatten (Flatten)            (None, 1024)              0         
    _________________________________________________________________
    dense (Dense)                (None, 1)                 1025      
    =================================================================
    Total params: 3,229,889
    Trainable params: 3,208,001
    Non-trainable params: 21,888
    _________________________________________________________________
    

# settings for binary classification (open / closed)



```python

new_model.compile( optimizer="adam",loss="categorical_crossentropy",metrics=["accuracy"])

```


```python
import keras
keras.__version__
# should have keras version 2.6.0 to work with tensorflow 2.7.0

```




    '2.6.0'




```python
import tensorflow as tf
print(tf.__version__)

```

    2.7.0
    


```python
history=new_model.fit(X,Y,epochs =30,validation_split= 0.3) ##training


```

    Epoch 1/30
    55/55 [==============================] - 410s 7s/step - loss: 5.5009e-08 - accuracy: 0.3507 - val_loss: 5.4287e-08 - val_accuracy: 0.5752
    Epoch 2/30
    55/55 [==============================] - 327s 6s/step - loss: 5.5009e-08 - accuracy: 0.3415 - val_loss: 5.4287e-08 - val_accuracy: 0.5819
    Epoch 3/30
    55/55 [==============================] - 336s 6s/step - loss: 5.5009e-08 - accuracy: 0.3455 - val_loss: 5.4287e-08 - val_accuracy: 0.5300
    Epoch 4/30
    55/55 [==============================] - 364s 7s/step - loss: 5.5009e-08 - accuracy: 0.3609 - val_loss: 5.4287e-08 - val_accuracy: 0.4381
    Epoch 5/30
    55/55 [==============================] - 352s 6s/step - loss: 5.5009e-08 - accuracy: 0.3512 - val_loss: 5.4287e-08 - val_accuracy: 0.3981
    Epoch 6/30
    55/55 [==============================] - 335s 6s/step - loss: 5.5009e-08 - accuracy: 0.3421 - val_loss: 5.4287e-08 - val_accuracy: 0.3901
    Epoch 7/30
    55/55 [==============================] - 325s 6s/step - loss: 5.5009e-08 - accuracy: 0.3461 - val_loss: 5.4287e-08 - val_accuracy: 0.3648
    Epoch 8/30
    55/55 [==============================] - 340s 6s/step - loss: 5.5009e-08 - accuracy: 0.3575 - val_loss: 5.4287e-08 - val_accuracy: 0.3582
    Epoch 9/30
    55/55 [==============================] - 324s 6s/step - loss: 5.5009e-08 - accuracy: 0.3524 - val_loss: 5.4287e-08 - val_accuracy: 0.3648
    Epoch 10/30
    55/55 [==============================] - 298s 5s/step - loss: 5.5009e-08 - accuracy: 0.3558 - val_loss: 5.4287e-08 - val_accuracy: 0.3702
    Epoch 11/30
    55/55 [==============================] - 294s 5s/step - loss: 5.5009e-08 - accuracy: 0.3587 - val_loss: 5.4287e-08 - val_accuracy: 0.3648
    Epoch 12/30
    55/55 [==============================] - 293s 5s/step - loss: 5.5009e-08 - accuracy: 0.3461 - val_loss: 5.4287e-08 - val_accuracy: 0.3622
    Epoch 13/30
    55/55 [==============================] - 304s 6s/step - loss: 5.5009e-08 - accuracy: 0.3569 - val_loss: 5.4287e-08 - val_accuracy: 0.3648
    Epoch 14/30
    55/55 [==============================] - 293s 5s/step - loss: 5.5009e-08 - accuracy: 0.3649 - val_loss: 5.4287e-08 - val_accuracy: 0.3688
    Epoch 15/30
    55/55 [==============================] - 323s 6s/step - loss: 5.5009e-08 - accuracy: 0.3552 - val_loss: 5.4287e-08 - val_accuracy: 0.3702
    Epoch 16/30
    55/55 [==============================] - 294s 5s/step - loss: 5.5009e-08 - accuracy: 0.3455 - val_loss: 5.4287e-08 - val_accuracy: 0.3662
    Epoch 17/30
    55/55 [==============================] - 308s 6s/step - loss: 5.5009e-08 - accuracy: 0.3644 - val_loss: 5.4287e-08 - val_accuracy: 0.3662
    Epoch 18/30
    55/55 [==============================] - 292s 5s/step - loss: 5.5009e-08 - accuracy: 0.3666 - val_loss: 5.4287e-08 - val_accuracy: 0.3662
    Epoch 19/30
    55/55 [==============================] - 309s 6s/step - loss: 5.5009e-08 - accuracy: 0.3455 - val_loss: 5.4287e-08 - val_accuracy: 0.3702
    Epoch 20/30
    55/55 [==============================] - 328s 6s/step - loss: 5.5009e-08 - accuracy: 0.3472 - val_loss: 5.4287e-08 - val_accuracy: 0.3688
    Epoch 21/30
    55/55 [==============================] - 337s 6s/step - loss: 5.5009e-08 - accuracy: 0.3575 - val_loss: 5.4287e-08 - val_accuracy: 0.3688
    Epoch 22/30
    55/55 [==============================] - 298s 5s/step - loss: 5.5009e-08 - accuracy: 0.3352 - val_loss: 5.4287e-08 - val_accuracy: 0.3648
    Epoch 23/30
    55/55 [==============================] - 295s 5s/step - loss: 5.5009e-08 - accuracy: 0.3541 - val_loss: 5.4287e-08 - val_accuracy: 0.3675
    Epoch 24/30
    55/55 [==============================] - 295s 5s/step - loss: 5.5009e-08 - accuracy: 0.3558 - val_loss: 5.4287e-08 - val_accuracy: 0.3702
    Epoch 25/30
    55/55 [==============================] - 293s 5s/step - loss: 5.5009e-08 - accuracy: 0.3484 - val_loss: 5.4287e-08 - val_accuracy: 0.3675
    Epoch 26/30
    55/55 [==============================] - 297s 5s/step - loss: 5.5009e-08 - accuracy: 0.3489 - val_loss: 5.4287e-08 - val_accuracy: 0.3675
    Epoch 27/30
    55/55 [==============================] - 302s 5s/step - loss: 5.5009e-08 - accuracy: 0.3427 - val_loss: 5.4287e-08 - val_accuracy: 0.3688
    Epoch 28/30
    55/55 [==============================] - 342s 6s/step - loss: 5.5009e-08 - accuracy: 0.3512 - val_loss: 5.4287e-08 - val_accuracy: 0.3715
    Epoch 29/30
    55/55 [==============================] - 315s 6s/step - loss: 5.5009e-08 - accuracy: 0.3444 - val_loss: 5.4287e-08 - val_accuracy: 0.3702
    Epoch 30/30
    55/55 [==============================] - 289s 5s/step - loss: 5.5009e-08 - accuracy: 0.3449 - val_loss: 5.4287e-08 - val_accuracy: 0.3742
    


```python
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(accuracy))

plt.plot(epochs, accuracy, "b", label="trainning accuracy")
plt.plot(epochs, val_accuracy, "r", label="validation accuracy")
plt.legend()
plt.show()

plt.plot(epochs, loss, "b", label="trainning loss")
plt.plot(epochs, val_loss, "r", label="validation loss")
plt.legend()
plt.show()
```


    
![png](output_48_0.png)
    



    
![png](output_48_1.png)
    



```python

                                                         
plt.plot(history.history['val_loss'],'r',history.history['val_loss'],'b')
plt.grid()
```


    
![png](output_49_0.png)
    



```python
new_model.save('my_model.h5')
```


```python
new_model = tf.keras.models.load_model('my_model.h5')
#new_model = tf.keras.models.load_model('my_model.h5')
```


```python
prediction = new_model.predict(test_x)
```


```python
prediction
```




    array([[-1.7289872],
           [-1.945787 ],
           [-1.6733572],
           [-1.6334705],
           [-1.6882563],
           [-1.6689553],
           [-1.6803803],
           [-1.8373342],
           [-1.6640165],
           [-1.7725499],
           [-1.7965825],
           [-1.783999 ],
           [-1.6702852],
           [-1.5382457],
           [-1.6308346],
           [-1.789535 ],
           [-1.5969024],
           [-1.7389499],
           [-1.7802289],
           [-1.8894086],
           [-1.6746356],
           [-1.543225 ],
           [-1.781309 ],
           [-1.8151345],
           [-1.6570253],
           [-1.9014244],
           [-1.7003114],
           [-1.7408102],
           [-1.6962793],
           [-1.6878645],
           [-1.9532851],
           [-1.554445 ],
           [-1.8092475],
           [-1.5455174],
           [-1.9948796],
           [-1.6638267],
           [-1.6307294],
           [-1.818531 ],
           [-1.7066208],
           [-1.5907323],
           [-1.7099599],
           [-1.8952122],
           [-1.4899721],
           [-1.9744735],
           [-1.6897192],
           [-1.625928 ],
           [-1.7776983],
           [-1.7039595],
           [-1.8704264],
           [-1.5410197],
           [-1.6612457],
           [-1.7489793],
           [-1.8835759],
           [-1.5520773],
           [-1.9877391],
           [-1.7501199],
           [-1.8459547],
           [-1.9217416],
           [-1.6528088],
           [-1.6283782],
           [-1.7285148],
           [-1.71566  ],
           [-1.7513957],
           [-1.7752569],
           [-1.7750281],
           [-1.9300649],
           [-2.0040839],
           [-1.9584767],
           [-1.802258 ],
           [-1.5858912],
           [-1.7008978],
           [-1.9809852],
           [-1.7829986],
           [-1.9863856],
           [-1.7822822],
           [-1.6509147],
           [-1.9061642],
           [-1.508528 ],
           [-1.712674 ],
           [-1.7910604],
           [-1.8012938],
           [-1.7245173],
           [-1.8193552],
           [-1.6867557],
           [-1.5624743],
           [-1.7844863],
           [-1.8331894],
           [-1.8060857],
           [-1.9446626],
           [-1.8396481],
           [-1.7413816],
           [-1.7921956],
           [-1.6931074],
           [-1.8444151],
           [-1.8020151],
           [-1.8287659],
           [-1.5405209],
           [-1.928575 ],
           [-1.7630105],
           [-1.7883607],
           [-1.7088699],
           [-1.5844195],
           [-1.5627215],
           [-1.6688898],
           [-1.5577642],
           [-1.8732928],
           [-1.7376215],
           [-1.9758672],
           [-1.9458554],
           [-1.8944514],
           [-1.7045957],
           [-1.62873  ],
           [-1.5886662],
           [-1.6762395],
           [-1.8176287],
           [-1.7644008],
           [-1.9923368],
           [-1.8712251],
           [-1.7919217],
           [-1.741258 ],
           [-1.7963965],
           [-1.6304096],
           [-1.8339458],
           [-1.870854 ],
           [-1.6000295],
           [-1.6263556],
           [-1.7520878],
           [-1.7776257],
           [-1.5674901],
           [-1.8019037],
           [-1.7147833],
           [-1.6437252],
           [-1.7826556],
           [-1.7898701],
           [-1.572189 ],
           [-1.8318714],
           [-1.9256799],
           [-1.6263396],
           [-1.896616 ],
           [-1.7629133],
           [-1.9966779],
           [-1.6530135],
           [-1.7772281],
           [-1.9257363],
           [-1.6799886],
           [-1.7160202],
           [-1.9038724],
           [-1.8434358],
           [-1.876416 ],
           [-1.6004231],
           [-1.6963123],
           [-1.6284683],
           [-1.7581282],
           [-1.5755904],
           [-1.7816724],
           [-1.6493318],
           [-1.6868285],
           [-1.6187341],
           [-2.1860843],
           [-1.6534259],
           [-1.8694248],
           [-1.5248684],
           [-1.7808825],
           [-1.7563517],
           [-1.5300486],
           [-1.8475469],
           [-1.6259559],
           [-1.7634301],
           [-1.9223194],
           [-1.8643763],
           [-1.6241822],
           [-1.7222719],
           [-1.7758442],
           [-1.8186698],
           [-1.8540413],
           [-1.6112517],
           [-1.8255961],
           [-1.5816511],
           [-1.7271638],
           [-1.6753619],
           [-1.7460907],
           [-1.758334 ],
           [-1.6314434],
           [-1.6743646],
           [-1.7940954],
           [-1.3285042],
           [-1.6103449],
           [-1.7187599],
           [-1.9571995],
           [-1.7619696],
           [-1.6905351],
           [-2.1184816],
           [-1.6377875],
           [-1.8192883],
           [-1.7472421],
           [-1.7570496],
           [-1.8519903],
           [-1.6540725],
           [-1.7314963],
           [-1.9031206],
           [-1.6821002],
           [-1.9645077],
           [-2.0823002],
           [-1.8245373],
           [-1.7913575],
           [-1.5346329],
           [-1.7713187],
           [-1.6545115],
           [-1.6521001],
           [-1.5848844],
           [-1.7018259],
           [-1.7620988],
           [-1.6378455],
           [-1.7933803],
           [-1.6798509],
           [-1.9286621],
           [-1.7164507],
           [-1.782129 ],
           [-1.6996137],
           [-1.9587406],
           [-1.7557979],
           [-1.7590804],
           [-1.7433257],
           [-1.7119498],
           [-1.5373402],
           [-1.6631871],
           [-1.7262802],
           [-1.7925867],
           [-1.7571955],
           [-1.6074574],
           [-1.6457982],
           [-1.7568879],
           [-1.5118794],
           [-1.7539706],
           [-1.7284205],
           [-1.6969007],
           [-1.7756679],
           [-1.6102562],
           [-1.5549941],
           [-1.7684972],
           [-1.8342683],
           [-1.7316368],
           [-1.6169543],
           [-1.6364541],
           [-1.5593729],
           [-1.5552373],
           [-1.7225282],
           [-1.7501583],
           [-2.014012 ],
           [-1.8412175],
           [-1.7952771],
           [-2.0185442],
           [-1.6941144],
           [-1.9632168],
           [-2.0646214],
           [-1.7940054],
           [-1.7343726],
           [-1.9550955],
           [-1.7327112],
           [-1.5900452],
           [-1.8320897],
           [-1.6464322],
           [-1.8990171],
           [-1.7058433],
           [-1.9334698],
           [-1.9304808],
           [-1.6468004],
           [-1.7242069],
           [-1.7254889],
           [-1.5909336],
           [-1.591124 ],
           [-1.8939176],
           [-1.6387446],
           [-1.671319 ],
           [-1.8570344],
           [-1.8403577],
           [-1.7223811],
           [-1.5421207],
           [-1.6822772],
           [-1.4582932],
           [-1.751523 ],
           [-1.8402393],
           [-1.7487488],
           [-1.7945237],
           [-1.9431878],
           [-1.8397903],
           [-1.7144544],
           [-1.8422952],
           [-1.6770236],
           [-1.6871554],
           [-1.8619723],
           [-1.7133777],
           [-1.5874441],
           [-1.6045846],
           [-1.6930101],
           [-1.7202013],
           [-1.5701964],
           [-1.6830008],
           [-1.5786632],
           [-1.5927701],
           [-1.6469129],
           [-1.7181312],
           [-1.6382067],
           [-1.5564951],
           [-1.8096545],
           [-1.7959547],
           [-1.6780518],
           [-1.8799443],
           [-1.7936568],
           [-1.7144542],
           [-1.8583755],
           [-1.7758181],
           [-1.812784 ],
           [-1.9761112],
           [-1.9603248],
           [-1.845259 ],
           [-1.7631294],
           [-1.8289001],
           [-1.6251333],
           [-1.6581726],
           [-1.6654571],
           [-1.6729221],
           [-1.8376131],
           [-1.9585326],
           [-1.7365804],
           [-1.6240005],
           [-1.695704 ],
           [-1.7509187],
           [-1.8563789],
           [-1.6254737],
           [-1.7486899],
           [-1.9319369],
           [-1.7542675],
           [-1.5309124],
           [-1.7761403],
           [-1.5731225],
           [-1.651353 ],
           [-1.4896512],
           [-1.5114827],
           [-1.8993386],
           [-2.0627341],
           [-1.7293069],
           [-1.955566 ],
           [-1.8185843],
           [-1.6469793],
           [-1.7174828],
           [-1.7738796],
           [-1.6292346],
           [-1.721828 ],
           [-1.629631 ],
           [-1.6166315],
           [-1.8468022],
           [-1.683378 ],
           [-1.8710289],
           [-1.7726829],
           [-1.642631 ],
           [-1.6770104],
           [-1.563393 ],
           [-1.7438093],
           [-1.6603061],
           [-1.8701763],
           [-1.806119 ],
           [-1.8531344],
           [-1.7051613],
           [-1.9155531],
           [-1.71052  ],
           [-1.6914654],
           [-1.7927698],
           [-1.7282162],
           [-1.6142216],
           [-1.5776542],
           [-1.6518431],
           [-1.7214998],
           [-1.9531748],
           [-1.6626288],
           [-1.6372986],
           [-1.7207131],
           [-1.8138269],
           [-1.7109147],
           [-1.6315148],
           [-1.6986125],
           [-1.9247127],
           [-1.6923773],
           [-1.729799 ],
           [-1.7653048],
           [-1.9342487],
           [-1.5386959],
           [-1.7333432],
           [-1.9455984],
           [-1.7195766],
           [-1.7130446],
           [-1.8264511],
           [-2.1469626],
           [-1.6911407],
           [-1.8388231],
           [-1.5435739],
           [-1.721369 ],
           [-1.6269662],
           [-1.864903 ],
           [-1.6889796],
           [-1.4932832],
           [-1.589205 ],
           [-1.7046896],
           [-1.5460141],
           [-1.5725672],
           [-1.8955816],
           [-1.618974 ],
           [-1.9644578],
           [-1.7957165],
           [-1.8435191],
           [-1.8633833],
           [-1.6329194],
           [-1.7542907],
           [-1.9226282],
           [-1.87504  ],
           [-1.8352565],
           [-1.8136752],
           [-1.8475413],
           [-1.7401013],
           [-1.611046 ],
           [-1.5186733],
           [-1.6833665],
           [-1.5267401],
           [-1.7539911],
           [-1.7971224],
           [-1.7758377],
           [-1.7391659],
           [-1.730965 ],
           [-1.6828694],
           [-1.774962 ],
           [-1.8382251],
           [-1.5913429],
           [-1.9935164],
           [-1.7556062],
           [-1.6363232],
           [-1.7052469],
           [-1.7544596],
           [-1.544703 ],
           [-1.8608983],
           [-1.7169521],
           [-2.0303867],
           [-1.6546007],
           [-1.8678043],
           [-1.6990423],
           [-1.7407265],
           [-1.8748469],
           [-1.7202379],
           [-1.6385021],
           [-1.5947254],
           [-1.8407612],
           [-1.6441146],
           [-1.7136935],
           [-1.8444821],
           [-1.7212019],
           [-1.733994 ],
           [-1.6111847],
           [-1.5716892],
           [-1.8130231],
           [-1.8239996],
           [-1.6303487],
           [-1.73013  ],
           [-1.5041211],
           [-1.4634883],
           [-1.8206806],
           [-1.6871768],
           [-1.7384129],
           [-1.6730345],
           [-1.6536021],
           [-1.6875682],
           [-1.5591342],
           [-1.7158754],
           [-1.7220895],
           [-1.9617237],
           [-1.8804064],
           [-1.599525 ],
           [-1.8349135],
           [-1.5506959],
           [-1.6347749],
           [-1.8271914],
           [-1.8940547],
           [-1.8308959],
           [-1.6115758],
           [-1.8420643],
           [-1.6536119],
           [-1.7228669],
           [-1.733552 ],
           [-1.7719351],
           [-1.6788374],
           [-1.9222696],
           [-1.7779899],
           [-1.7815475],
           [-1.712591 ],
           [-1.4763215],
           [-1.6683314],
           [-1.7123213],
           [-1.7400981],
           [-1.8456355],
           [-1.9910153],
           [-1.5285308],
           [-1.8099121],
           [-1.6192968],
           [-1.6382995],
           [-1.748635 ],
           [-1.7303679],
           [-1.8510271],
           [-1.5396996],
           [-1.8855746],
           [-1.5673618],
           [-1.7892859],
           [-1.600654 ],
           [-1.6239846],
           [-1.9680166],
           [-1.640187 ],
           [-1.7392333],
           [-2.1849527],
           [-1.6586094],
           [-1.8206524],
           [-1.735994 ],
           [-2.115666 ],
           [-1.7284585],
           [-2.0513487],
           [-1.5589379],
           [-1.5677824],
           [-1.6953766],
           [-1.5939798],
           [-1.7683814],
           [-1.7243179],
           [-1.4427266],
           [-1.6007786],
           [-1.5990834],
           [-1.8315332],
           [-1.7157032],
           [-1.841373 ],
           [-1.916127 ],
           [-1.8422989],
           [-1.6717179],
           [-1.6640966],
           [-1.8608181],
           [-1.713292 ],
           [-1.6427569],
           [-1.7334602],
           [-1.7701836],
           [-1.9137708],
           [-1.626306 ],
           [-1.5100417],
           [-1.6749252],
           [-1.7521405],
           [-1.8314703],
           [-1.9044815],
           [-1.7016158],
           [-1.7745566],
           [-1.8080988],
           [-1.660083 ],
           [-1.6009154],
           [-1.7186317],
           [-1.518264 ],
           [-1.5661077],
           [-1.9060616],
           [-1.8596922],
           [-1.6899068],
           [-1.7438179],
           [-1.8276187],
           [-1.711968 ],
           [-1.6904355],
           [-1.5502672],
           [-1.7886003],
           [-1.8519679],
           [-1.8195773],
           [-1.6418803],
           [-1.4785576],
           [-1.699336 ],
           [-1.6451639],
           [-1.7391818],
           [-1.7324595],
           [-1.7057872],
           [-1.9620495],
           [-1.7772979],
           [-1.8239622],
           [-1.7298529],
           [-1.6417677],
           [-1.9134184],
           [-1.7796909],
           [-1.5773555],
           [-1.8753006],
           [-1.8240321],
           [-1.7146177],
           [-1.5675046],
           [-1.7118359],
           [-1.8174654],
           [-1.9863881],
           [-1.5368266],
           [-1.7387726],
           [-1.582766 ],
           [-1.6648529],
           [-1.8935659],
           [-1.845521 ],
           [-1.5575373],
           [-1.6355846],
           [-1.8779614],
           [-2.1197617],
           [-1.7969778],
           [-1.7278681],
           [-1.9076467],
           [-1.7459298],
           [-1.6373706],
           [-1.4948875],
           [-1.5457551],
           [-1.7607223],
           [-1.6700419],
           [-2.058998 ],
           [-1.7863812],
           [-1.8145286],
           [-1.746879 ],
           [-1.820797 ],
           [-1.7697963],
           [-1.735171 ],
           [-1.6272941],
           [-1.5291104],
           [-1.8244131],
           [-1.7804273],
           [-1.8877783],
           [-1.5779045],
           [-1.614636 ],
           [-1.7622535],
           [-1.9273546],
           [-1.8768761]], dtype=float32)



# checking the network for predictions



```python
ing_array = cv2.imread('CLOSE.png', cv2.IMREAD_GRAYSCALE)
backtorgb = cv2.cvtColor(ing_array, cv2.COLOR_GRAY2RGB)
new_array= cv2.resize(backtorgb, (ing_size, ing_size))
```


```python
plt.imshow(new_array)
```




    <matplotlib.image.AxesImage at 0x24d2346fa00>




    
![png](output_56_1.png)
    



```python
X_input = np.array(new_array).reshape(1, ing_size, ing_size, 3) 

```


```python
X_input.shape

```




    (1, 224, 224, 3)




```python
plt.imshow(new_array)
```




    <matplotlib.image.AxesImage at 0x24d235275e0>




    
![png](output_59_1.png)
    



```python
X_input=X_input/255.0

```


```python
prediction = new_model.predict(X_input)
if (prediction>0):
    print("opend eyes")
    print(prediction)
else:
    print("closed eyes")
    print(prediction)


```


```python
prediction[:,0]
```


```python


```


```python

```


```python
norm_predict =  float('{:f}'.format(prediction[0][0]))
```


```python
norm_predict

```

# Lets check on unknown Images



```python
img=cv2.imread('12.jpg')
```


```python
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

```


```python
faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

```


```python
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
```


```python
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 

```


```python
eyes = eye_cascade.detectMultiScale(gray, 1.1,4)

```


```python
for(x, y, w, h) in eyes:
    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
```


```python
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
```

# cropping the eyes


```python
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_eye.xml')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
   #print (facecascade.empty())
eyes = eye_cascade.detectMultiScale(gray,1.1,4)
for x,y,w,h in eyes:
    roi_gray = gray[y:y+h, x:x+w]
    roi_color = img[y:y+h, x:x+w]
    eyess = eye_cascade.detectMultiScale(roi_gray)
    if len(eyess) == 0:
        print("eyes are not detected")
    else:
        for (ex,ey, ew, eh) in eyess:
            eyes_roi = roi_color[ey: ey+eh, ex:ex + ew]
```


```python
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
```


```python
plt.imshow(cv2.cvtColor(eyes_roi, cv2.COLOR_BGR2RGB))

```


```python
eyes_roi.shape
```


```python
final_image =cv2.resize(eyes_roi, (224,224))
final_image = np.expand_dims(final_image, axis=0) ## need fourth dimension
final_image= final_image/255.0


```


```python
final_image.shape

```


```python
pp=new_model.predict (final_image)
if (pp>0):
    print("opend eyes")
    print(pp)
else:
    print("closed eyes")
    print(pp)
```


```python

```


```python

```


```python
"""import cv2 ### pip install opencv-python

## pip instalL opencv-contrib-python ##    fullpackage
##from deepface import DeepFace pip install deepface
path = "haarcascade_frontalface_default.xml"
FaceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

cap = cv2.VideoCapture(1)
 ## Check if the webcam is opened correctly
if not cap.isOpened ():
    cap = cv2.VideoCapture(0)
if not cap.isOpened ():
    raise IOError("Cannot open webcan")
while True:
    ret, frame = cap.read()
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    #print(faceCascade. empty())
    eyes = eye_cascade.detectMultiScale(gray, 1.1,4)
    for x,y,w,h in eyes:
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = frame[y:y+h, x:x+w]
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        eyess= eye_cascade.detectMultiScale(roi_gray)
        if len(eyess) == 0:
             print("eyes are not detected")
        else:
                
            for (ex, ey, ew, eh) in eyess:
                eyes_roi = roi_color[ey: ey+eh, ex:ex + ew]
    final_image =cv2.resize(eyes_roi, (224,224))
    final_image = np.expand_dims (final_image, axis =0) # need fourth dimension
    final_image=final_image/255.0
    Predictions = new_model.predict(final_image)
    
    if (Predictions>0.084263):
        print(Predictions)
        status = "Open Eyes"
    else:
        print(Predictions)
        status = "Closed Eyes"
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    print(faceCascade.empty())
    faces = faceCascade.detectMultiScale(gray, 1.1,4)
   # Draw a rectangle around the faces
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
    font = cv2.FONT_HERSHEY_SIMPLEX
# Use putText() method for
# inserting text on video
    cv2.putText(frame,
                status,
                (50, 50),
                font, 3,
                (0, 0, 255),
                2,
                cv2. LINE_4)
    cv2.imshow( 'Drowsiness Detection ', frame)
    if cv2.waitKey(2) & 0XFF == ord('q'):
         break
cap.release()
cv2.destroyAllwindows ()"""
```

#   if eyes are cloÅŸed for unusual time, like more than blinks, for few seconds,alarm Generated


```python

import winsound
frequency = 2500# Set Frequency To 25e0 Hertz
duration = 1000# Set Duration To 1000 ms == 1 second       
import numpy as np
import cv2 ### pip install opencv-python
        ## pip install opencv-contrib-python fullpackage
       #from deepface import DeepFace ## pip install deepface
path = "haarcascade_frontalface_default.xml"
faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
cap = cv2.VideoCapture(1)
       # Check if the webcam is opened correctly
if not cap.isOpened():
    cap = cv2.VideoCapture(0)
if not cap.isOpened():
    raise IOError("Cannot open webcam")
counter = 0
while True:
    ret, frame = cap.read()
    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_eye_tree_eyeglasses.xml')
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
          #print(facecascade.empty())
    eyes = eye_cascade.detectMultiScale(gray,1.1,4)
    for x,y,w,h in eyes:
        roi_gray = gray[y:y+h, x:x+w]
        roi_color = frame[y:y+h, x:x+w]
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        eyess = eye_cascade.detectMultiScale(roi_gray)
        if len(eyess) == 0:
            print("eyes are not detected")
        else:
            for (ex,ey,ew, eh) in eyess:
                eyes_roi = roi_color[ey: ey+eh, ex:ex + ew]
    gray = cv2.cvtColor (frame, cv2.COLOR_BGR2GRAY)
    print(faceCascade.empty())
    faces = faceCascade.detectMultiScale(gray, 1.1,4)
    
  # Draw a rectangle around the faces
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
    font = cv2.FONT_HERSHEY_SIMPLEX
## Use putText() method for
#+ inserting text on video
    final_image=cv2.resize(eyes_roi,(224,224))  
    final_image = np.expand_dims(final_image, axis =0) ## need fourth dimension
    final_image=final_image/255.0
    Predictions = new_model.predict(final_image)
    if ((Predictions*100)>0 ):
        print(Predictions)
        status = "Open Eyes"
        cv2.putText(frame,
                status,
                (150, 150),
                font, 3,
                (0, 255, 0),
                2,
                cv2. LINE_4)
        x1, y1, w1, h1 = 0,0,175,75
   # Draw black background rectangle
        cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)
  ## Add text
        cv2. putText(frame, 'Active', (x1 + int(w1/10),y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255, ), 2)
    else:
        counter = counter + 1
        status = "Closed Eyes"
        cv2.putText(frame,
                status,
                (150,150),
                font,3,
                (0, 0, 255),
                2,
                cv2.LINE_4)
    
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)
        if counter>5:
            x1,y1,w1, h1 =0,0,175,75
            # Draw black background rectangle
            cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)
            # Add text
            cv2.putText (frame, 'Sleep Alert !!', (x1 + int(w1/10),y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)
            winsound.Beep(frequency, duration)
            counter =0
    cv2.imshow( 'Drowsiness Detection ', frame)
    if cv2.waitKey(2) & 0XFF == ord('q'):
        break
cap.release()
cv2.destroyAllwindows ()
```

# by MOUHIHA Mohamed
